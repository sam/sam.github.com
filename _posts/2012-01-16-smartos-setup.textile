---
layout: post
title: SmartOS Setup
published: false
---

Here we're going to go over the basics of getting the latest "SmartOS release":http://wiki.smartos.org/display/DOC/Download+SmartOS (January 12, 2012 as of this post) running. Here's a screen-cast for the video inclined, but I'll also provide all the tweaky bits with some more detail below so you can follow along at home.

You'll need an *Intel* CPU with the *VT-x* extensions assuming you want to use KVM virtualization on SmartOS as it doesn't support AMD CPUs. You'll get VT-x support on any 2nd Gen i3 processor, the i5 or i7 processors, or a Nehalem or later Xeon. This should cover most any Intel system you might have at your office as long as it's not more than a couple years old. You can find out for sure on "Intel's Virtualization Technology List page":http://ark.intel.com/VTList.aspx.

If you're not already familiar with ZFS, I'd suggest downloading the latest "FreeBSD":http://freebsd.org, running it under VirtualBox, and playing around with it there. For 99% of what you'll be doing, aside from standard device-name conventions for disks, it'll be identical to what you'll be doing in SmartOS. You might also want to buff up on the "Wikipedia article on ZFS":http://en.wikipedia.org/wiki/ZFS.

For everyone else, on with the show!

<embed the youtubes screencast>

h3. Single Disk Configurations

First, an issue with SmartOS configuration common to all installations. *No swap is created.* If you create a _zones/swap_ device, it'll be used automatically at boot. The device should be a _zvol_, the same size as your physical memory. So in my case:

<script src="https://gist.github.com/1622208.js?file=gistfile1.sh"></script>

The _primarycache_ setting is your ZFS Adaptive Read Cache (in-memory cache). It obviously doesn't make sense to cache data stored on a swap device since if you had memory to cache it, you'd be better served using that memory for the data directory instead of pushing it out to disk.

The _secondarycache_ is the ZFS L2ARC, ie: a cache device (SSD) attached to the pool. Again, caching swap doesn't really make much sense, so make sure that it's not.

I've seen it mentioned on IRC that disabling the caches for swap is automatic, but without confirmation, so no harm in making sure.

h3. Multiple Disk Configurations

Now we move onto multi-disk configurations. If this is you, then hold off on adding the swap above. We'll get around to it in a bit after a few other things. The SmartOS configuration script has a bug that ignores extra disks you pass it. It'll only use the first.

On our staging system we have a JBOD with 24 drives on it, and 3 drives in the server itself. We want to use that JBOD for our _zones_ zpool. Because *you can't remove a top-level vdev in ZFS*, you're in a bit of a pickle though. If you use one of those disks right off the bat, you won't be able to get the end result/configuration you want. *Unless* the disk you use is ultimately intended to be a spare as you can add/remove those at will.

_So if you've already configured SmartOS, you want a redundant multi-disk configuration (mirrored, raidz, etc) and you don't have a spare disk, this is probably where you stop and go dig up some old disks you can temporarily use while we shuffle things around._

If you've already configured your install, and you chose one of the disks you ultimately want to use in your pool, then you'll need to delete it, and reconfigure it to use one of your spare disks instead.

The easiest way to blow the pool away so you can get back to the configuration is to run the @format@ command, and blow away the partition on the disk with fdisk.

*Making Things Complicated:* As an alternative to the fdisk/reboot, you could swap out the disk by adding your spare as a mirror, then removing the original disk like so:

<script src="https://gist.github.com/1622225.js?file=gistfile1.sh"></script>

Once that's done and your running setup is on a spare disk, we can continue.

First up, you can't configure a dump device on your pool if:

# It's going to include log or cache devices
# If it's going to span multiple raidz vdevs

So you're going to want to dedicate a raw physical device. As an aside, because of an issue in a critical service (@vmadmd@) when you have more than one zpool, you can't put the dump device on a secondary pool. You're going to want to make sure your system only has the "zones" pool when all is said and done, so that's why we use a raw device here.

<script src="https://gist.github.com/1622239.js?file=gistfile1.sh"></script>

You can't use this device for anything now else mind you. So in the multi-disk setup you need:

# The initial disk you'll install to (this could be a device you don't intend to use, or it could ultimately be intended to be add to the pool as a spare)
# The disk for the dump device
# The disk(s) for your multi-device "zones" zpool

Now we'll create the multi-device zpool that we'll eventually turn into our "zones" zpool:

<script src="https://gist.github.com/1622242.js?file=gistfile1.sh"></script>

Note, this is a JBOD in my case. Each of these is a 1TB 7200RPM disk, so this will create about 15TB of usable space after subtracting the spares (3TB) and parity (6TB).

_It's really a bit of a silly setup for a production VM host._

If you need that much storage for guest content, it should probably be on NFS or something so you can eventually have redundant, load-balanced guests. If you needed it for performance, you'd use smaller, higher-RPM disks in conjunction with SSDs.

I'm just using this setup here because this is a host for our non-production staging sites. By comparison the production setup consists of twelve 10,000RPM 300GB disks (two of those reserved for spares, and the other ten split into two raidz sets), two mirrored 50GB SSDs for log devices, and one 100GB SSD as a L2ARC cache device. A lot less storage space, but a lot more performance (and guest _content_ is on NFS anyway, this is just for OS installs, swap, logs, etc). When you're running a lot of VMs, disk IO performance is going to be pretty high up there when it comes to keeping things snappy.

So now we have our "backup" zpool. Let's replicate our data to it:

<script src="https://gist.github.com/1622261.js?file=gistfile1.sh"></script>

Now that our dump device is off of the "zones" pool, and we've backed up our install, we can delete it:

<script src="https://gist.github.com/1622264.js?file=gistfile1.sh"></script>

Finally, we'll export our "backup" pool, and reimport it with the "zones" name:

<script src="https://gist.github.com/1622268.js?file=gistfile1.sh"></script>

If the disk for the original configure-time "zones" pool was intended as a spare, and you had left it off earlier in the @zpool create backup@ since it was in-use, now is when you'd add it back:

<script src="https://gist.github.com/1622273.js?file=gistfile1.sh"></script>

Now we'll loop back around to the issue we mentioned at the beginning, and create the swap device that the configuration script missed creating for us:

<script src="https://gist.github.com/1622208.js?file=gistfile1.sh"></script>

Finally your zpool is how you wanted it. All disks where they belong, and a dedicated dump device humming along. Go ahead and reboot now to ensure everything's properly mounted, and afterwards ensure that @swap -l@ lists your swap device and @dumpadm@ displays your dedicated device.

If you run a @zfs list@ you'll probably notice that the backup retained a _zones/dump_ file-system. That's no longer used since we moved it to a dedicated device, so let's do a little house-cleaning and delete it:

<script src="https://gist.github.com/1622281.js?file=gistfile1.sh"></script>

Ultimately we've ended up with something like this:

<script src="https://gist.github.com/1622279.js?file=gistfile1.sh"></script>

Now you're ready to rock, creating your own base image, and provisioning new guests. You'll find help on that in the SmartOS wiki article: "How to create a Virtual Machine in SmartOS":http://wiki.smartos.org/display/DOC/How+to+create+a+Virtual+Machine+in+SmartOS.